{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "import CNN_encoder as cn\n",
    "import functions as fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取圖片位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63325,), (450,), (450,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./dataset/train.txt', sep=' ', header=None)\n",
    "test_df = pd.read_csv('./dataset/test.txt', sep=' ', header=None)\n",
    "val_df = pd.read_csv('./dataset/val.txt', sep=' ', header=None)\n",
    "train_paths, train_response = './dataset/' + train_df[0].to_numpy(), train_df[1].to_numpy()\n",
    "test_paths, test_response = './dataset/' + test_df[0].to_numpy(), test_df[1].to_numpy()\n",
    "val_paths, val_response = './dataset/' + val_df[0].to_numpy(), val_df[1].to_numpy()\n",
    "train_paths.shape, test_paths.shape, val_paths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用CNN做特徵提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(fc)\n",
    "importlib.reload(cn)\n",
    "\n",
    "# 訓練用超參數\n",
    "batch_size = 20\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger = fc.create_logger('./logger/', 'encoder_logger.txt')\n",
    "model = cn.CNN_model_encoder().to(device)\n",
    "lossf = nn.MSELoss().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 15:16:11,102 INFO start at 1660202171.1022706\n",
      "2022-08-11 15:16:12,869 INFO train time 1.7543730735778809, 1/100\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "# model = cn.CNN_model_encoder().to(device)\n",
    "# model.load_state_dict(torch.load('./logger/CNN_model_encoder.pt'))\n",
    "\n",
    "start = time.time()\n",
    "logger.info('start at {}'.format(start))\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    img_index = 0\n",
    "    img_len = train_paths.shape[0]\n",
    "    while img_index < img_len:\n",
    "        try:\n",
    "            imgs = train_paths[img_index:img_index + batch_size]\n",
    "            pic = fc.path2pic(imgs)\n",
    "            inputs = fc.pic2tensor(pic).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss1 = lossf(outputs[0], outputs[9])\n",
    "            loss2 = lossf(outputs[1], outputs[8])\n",
    "            loss3 = lossf(outputs[2], outputs[7])\n",
    "            loss4 = lossf(outputs[3], outputs[6])\n",
    "            loss5 = lossf(outputs[4], outputs[5])\n",
    "            loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            img_index += batch_size\n",
    "            torch.save(model.state_dict(), './model/CNN_model_encoder.pt')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(img_index)\n",
    "            logger.info(e, exc_info=True)\n",
    "    \n",
    "    train_time = time.time() - start\n",
    "    torch.save(model.state_dict(), './model/CNN_model_encoder.pt')\n",
    "    logger.info('train time {}, {}/{}'.format(train_time, epoch, epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取圖片位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63325,), (450,), (450,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./dataset/train.txt', sep=' ', header=None)\n",
    "test_df = pd.read_csv('./dataset/test.txt', sep=' ', header=None)\n",
    "val_df = pd.read_csv('./dataset/val.txt', sep=' ', header=None)\n",
    "train_paths, train_response = './dataset/' + train_df[0].to_numpy(), train_df[1].to_numpy()\n",
    "test_paths, test_response = './dataset/' + test_df[0].to_numpy(), test_df[1].to_numpy()\n",
    "val_paths, val_response = './dataset/' + val_df[0].to_numpy(), val_df[1].to_numpy()\n",
    "train_paths.shape, test_paths.shape, val_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "importlib.reload(fc)\n",
    "importlib.reload(cn)\n",
    "\n",
    "# 訓練用超參數\n",
    "batch_size = 20\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger = fc.create_logger('./logger/', 'encoder_logger.txt')\n",
    "model = cn.CNN_model_encoder().to(device)\n",
    "model.load_state_dict(torch.load('./model/CNN_model_encoder.pt'))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-28 14:48:43,432 INFO img_index:20\n",
      "2021-03-28 14:48:45,217 INFO img_index:40\n",
      "2021-03-28 14:48:46,744 INFO img_index:60\n",
      "2021-03-28 14:48:48,383 INFO img_index:80\n",
      "2021-03-28 14:48:49,968 INFO img_index:100\n",
      "2021-03-28 14:48:51,656 INFO img_index:120\n",
      "2021-03-28 14:48:53,305 INFO img_index:140\n",
      "2021-03-28 14:48:54,893 INFO img_index:160\n",
      "2021-03-28 14:48:56,410 INFO img_index:180\n",
      "2021-03-28 14:48:57,928 INFO img_index:200\n",
      "2021-03-28 14:48:59,448 INFO img_index:220\n",
      "2021-03-28 14:49:00,968 INFO img_index:240\n",
      "2021-03-28 14:49:02,444 INFO img_index:260\n",
      "2021-03-28 14:49:03,977 INFO img_index:280\n",
      "2021-03-28 14:49:05,539 INFO img_index:300\n",
      "2021-03-28 14:49:07,053 INFO img_index:320\n",
      "2021-03-28 14:49:08,566 INFO img_index:340\n",
      "2021-03-28 14:49:10,099 INFO img_index:360\n",
      "2021-03-28 14:49:11,612 INFO img_index:380\n",
      "2021-03-28 14:49:13,143 INFO img_index:400\n",
      "2021-03-28 14:49:14,612 INFO img_index:420\n",
      "2021-03-28 14:49:16,164 INFO img_index:440\n",
      "2021-03-28 14:49:16,908 INFO img_index:460\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "img_index = 0\n",
    "img_len = val_paths.shape[0]\n",
    "features = []\n",
    "\n",
    "while img_index < img_len:\n",
    "    imgs = val_paths[img_index:img_index + batch_size]\n",
    "    pic = fc.path2pic(imgs)\n",
    "    inputs = fc.pic2tensor(pic).to(device)\n",
    "    outputs = model.feature_extract(inputs)\n",
    "    img_index += batch_size\n",
    "    features.append(outputs)\n",
    "    logger.info('img_index:{}'.format(img_index))\n",
    "feature_array = torch.cat(features, dim=0)\n",
    "feature_array_cpu = feature_array.clone().detach().cpu()\n",
    "torch.save(feature_array_cpu, './imgFeature/valFeature.pt')\n",
    "del(feature_array)\n",
    "del(feature_array_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5216"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_norm = './train/NORMAL/'\n",
    "imglist_norm = os.listdir(path_norm)\n",
    "path_pneu = './train/PNEUMONIA/'\n",
    "imglist_pneu = os.listdir(path_pneu)\n",
    "img_list = []\n",
    "for i in imglist_norm:\n",
    "    img_list.append(path_norm + i)\n",
    "    \n",
    "for i in imglist_pneu:\n",
    "    img_list.append(path_pneu + i)\n",
    "    \n",
    "random.shuffle(img_list)\n",
    "len(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 訓練用參數\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_model_encoder().to(device)\n",
    "lossf = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = img_list[0:10]\n",
    "pic = path2pic(imgs)\n",
    "inputs = pic2tensor(pic).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 512, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss5 = lossf(outputs[0].detach(), outputs[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss5.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = lossf(outputs[1].detach(), outputs[6])\n",
    "loss2 = lossf(outputs[2].detach(), outputs[5])\n",
    "loss3 = lossf(outputs[8].detach(), outputs[13])\n",
    "loss4 = lossf(outputs[9].detach(), outputs[12])\n",
    "loss5 = lossf(outputs[0].detach(), outputs[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6279, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " tensor(1.3595, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " tensor(1.4133, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " tensor(1.4666, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " tensor(0.8438, device='cuda:0', grad_fn=<MseLossBackward>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1, loss2, loss3, loss4, loss5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 12, 12])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n",
    "# non-square kernels and unequal stride and with padding\n",
    "# m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "output = m(input)\n",
    "# exact output size can be also specified as an argument\n",
    "input = torch.randn(1, 16, 12, 12)\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "h = downsample(input)\n",
    "h.size()\n",
    "torch.Size([1, 16, 6, 6])\n",
    "output = upsample(h, output_size=input.size())\n",
    "output.size()\n",
    "torch.Size([1, 16, 12, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.ConvTranspose2d(16, 33, 3, stride=1)\n",
    "input = torch.randn(20, 16, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 52, 102])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv2d(3, 1, (3, 3), stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(m.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 3, 3]), torch.Size([8]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m.parameters())[0].shape, list(m.parameters())[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(20, 3, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 48, 98])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.3824, 0.2618, 0.8074, 0.4320, 0.8458],\n",
       "           [0.4926, 0.1995, 0.1289, 0.6962, 0.2754],\n",
       "           [0.2103, 0.5003, 0.0784, 0.1984, 0.2855],\n",
       "           [0.9783, 0.1403, 0.6125, 0.6148, 0.9563],\n",
       "           [0.8009, 0.4543, 0.0335, 0.1047, 0.0306]]],\n",
       " \n",
       " \n",
       "         [[[0.2588, 0.2052, 0.7625, 0.1607, 0.3166],\n",
       "           [0.3952, 0.2438, 0.9578, 0.8956, 0.9670],\n",
       "           [0.6519, 0.7244, 0.5740, 0.9927, 0.9858],\n",
       "           [0.4974, 0.8008, 0.7410, 0.8023, 0.8763],\n",
       "           [0.3673, 0.3164, 0.7160, 0.3681, 0.2028]]]]),\n",
       " torch.Size([2, 1, 5, 5]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand(2, 1, 5, 5)\n",
    "input, input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.4926, 0.8074, 0.8074, 0.8458],\n",
       "           [0.5003, 0.5003, 0.6962, 0.6962],\n",
       "           [0.9783, 0.6125, 0.6148, 0.9563],\n",
       "           [0.9783, 0.6125, 0.6148, 0.9563]]],\n",
       " \n",
       " \n",
       "         [[[0.3952, 0.9578, 0.9578, 0.9670],\n",
       "           [0.7244, 0.9578, 0.9927, 0.9927],\n",
       "           [0.8008, 0.8008, 0.9927, 0.9927],\n",
       "           [0.8008, 0.8008, 0.8023, 0.8763]]]]),\n",
       " torch.Size([2, 1, 4, 4]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.AdaptiveMaxPool2d((4, 4))\n",
    "output = m(input)\n",
    "output, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LayerNorm(x.shape[2:], elementwise_affine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5829,  1.4797,  0.4275,  ...,  0.0336, -0.5549, -0.4246],\n",
       "          [ 0.1870, -0.1482,  0.5749,  ..., -1.0905, -1.6453,  0.7862],\n",
       "          [ 0.6661, -0.2262, -1.0174,  ..., -1.4119,  1.2582, -0.2301],\n",
       "          ...,\n",
       "          [ 1.3639,  0.3099,  0.6002,  ..., -0.6457,  1.5809,  1.2038],\n",
       "          [ 0.3407,  1.5708, -1.5265,  ...,  0.3097, -1.5203, -0.2271],\n",
       "          [ 0.0590, -1.6453, -0.5070,  ...,  0.0652, -0.1709, -0.0558]],\n",
       "\n",
       "         [[ 0.7759, -1.2816,  1.4900,  ...,  0.8405,  0.5381, -0.0728],\n",
       "          [ 0.2517,  0.4695, -1.1645,  ...,  0.2120,  0.8396, -1.4304],\n",
       "          [-0.7659, -0.2544,  0.8152,  ..., -0.2501,  0.8558, -0.1098],\n",
       "          ...,\n",
       "          [-0.3252,  1.3697, -0.7355,  ..., -1.3774,  1.4510, -0.4577],\n",
       "          [ 1.3610, -0.1968, -0.1480,  ...,  0.0546, -0.2145,  1.2643],\n",
       "          [-0.8530, -0.9829,  0.7633,  ...,  1.2993, -1.8078,  1.3453]],\n",
       "\n",
       "         [[-1.3533, -0.2200,  1.4388,  ..., -0.4057, -1.2009,  0.2431],\n",
       "          [ 0.5953, -0.6241,  1.4949,  ...,  0.9503,  1.5338, -0.1554],\n",
       "          [ 1.5310, -0.3656, -1.4939,  ..., -0.5093,  0.1569, -1.5182],\n",
       "          ...,\n",
       "          [-1.7803, -0.8592, -0.1061,  ...,  0.6132,  0.1895,  0.0881],\n",
       "          [ 0.8601, -1.0592, -1.6163,  ...,  1.7632,  0.1428, -0.8005],\n",
       "          [ 1.4485,  0.6076,  1.0932,  ..., -0.3053, -1.0017, -1.0762]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0208,  1.1878,  0.5449,  ..., -1.5339,  0.9473, -0.8470],\n",
       "          [ 1.2163,  1.0863,  0.3176,  ..., -0.0872,  0.9790,  0.4588],\n",
       "          [ 0.9697, -0.1771, -0.9882,  ..., -1.0857,  1.4836, -1.4453],\n",
       "          ...,\n",
       "          [-0.6816,  1.4120, -1.5358,  ...,  0.5511, -1.5203,  0.9829],\n",
       "          [ 0.7332,  1.3690, -0.3950,  ...,  0.8584, -1.7769,  0.8813],\n",
       "          [ 0.6682,  0.9933,  0.8276,  ..., -0.2405, -0.8871, -0.2552]],\n",
       "\n",
       "         [[ 1.4424,  0.3828, -1.3258,  ...,  0.3167, -1.5504, -1.2526],\n",
       "          [ 1.5605, -1.5999, -1.3618,  ..., -1.6440,  0.6156, -0.9854],\n",
       "          [-1.1350, -0.7187,  1.5542,  ..., -0.4388,  0.3044, -0.2160],\n",
       "          ...,\n",
       "          [-0.9230,  1.3820,  0.4121,  ..., -0.6884, -0.1581,  1.1345],\n",
       "          [-0.5382,  1.4310, -1.3618,  ...,  0.6326, -0.4093, -0.6291],\n",
       "          [-1.1385,  1.3393, -0.5633,  ..., -0.6221,  0.8959,  0.9231]],\n",
       "\n",
       "         [[ 0.6511, -0.4176, -0.7540,  ..., -1.3844, -0.2525, -0.6739],\n",
       "          [ 0.2173,  0.7375, -1.2675,  ..., -0.1158, -0.8005,  1.4811],\n",
       "          [ 1.2654, -0.3950,  0.4855,  ..., -0.3430,  1.1926,  0.2265],\n",
       "          ...,\n",
       "          [ 1.2337,  1.2960, -1.0348,  ...,  0.8826, -1.2619,  0.9064],\n",
       "          [ 1.1744, -0.0942, -1.4742,  ..., -0.3371,  1.0678, -0.7522],\n",
       "          [ 1.6298,  0.4692, -0.1290,  ...,  1.7273, -0.8176,  0.3104]]],\n",
       "\n",
       "\n",
       "        [[[-0.5054,  0.1325,  0.4262,  ...,  0.3739,  1.3096,  1.0176],\n",
       "          [ 0.2262,  1.4440,  1.6015,  ...,  1.5666,  1.1188,  0.2592],\n",
       "          [-1.2859, -0.4469, -1.6076,  ...,  0.9395,  1.0265, -0.8837],\n",
       "          ...,\n",
       "          [-1.0133, -0.0811,  0.6175,  ...,  1.2730, -1.5217,  0.8636],\n",
       "          [-0.9044,  1.0860, -0.5062,  ..., -1.5552,  1.4105,  0.6488],\n",
       "          [-0.4444,  0.5151,  1.4648,  ..., -1.0860,  0.0989, -1.2382]],\n",
       "\n",
       "         [[-0.9208,  1.3889,  0.7792,  ..., -0.0696,  1.2206, -0.0399],\n",
       "          [-0.9245, -0.0500,  0.4196,  ...,  0.4052,  0.6756,  1.1053],\n",
       "          [-0.9148, -0.7021,  0.6247,  ...,  0.5867,  0.5635,  0.3618],\n",
       "          ...,\n",
       "          [-0.2872,  0.3361, -0.9811,  ...,  1.7737,  1.4631,  1.5625],\n",
       "          [-1.2479,  0.5960, -0.0258,  ..., -0.4816, -0.4758, -1.4579],\n",
       "          [-0.2998, -1.0621,  0.7967,  ...,  0.5420, -0.6903,  1.0327]],\n",
       "\n",
       "         [[-1.2661, -0.6415, -0.6524,  ...,  1.3905, -0.0721,  0.1979],\n",
       "          [-0.6425,  1.0530, -0.0320,  ..., -0.7377,  0.8073, -0.4400],\n",
       "          [ 0.4605, -1.7244,  0.3636,  ..., -0.8037,  1.2537,  0.5579],\n",
       "          ...,\n",
       "          [ 0.3563,  1.1776,  0.3295,  ..., -0.7753,  0.4150,  0.1893],\n",
       "          [-1.3423, -0.0464, -1.4034,  ...,  1.3457,  1.4368, -0.3100],\n",
       "          [ 1.7015, -1.5308, -0.4198,  ...,  0.4286,  0.7002,  1.3761]]],\n",
       "\n",
       "\n",
       "        [[[-0.1558, -1.8375,  0.4914,  ..., -1.8799,  1.1173, -0.9250],\n",
       "          [-0.0126, -0.2624,  0.1127,  ...,  0.9589,  1.6580,  0.6192],\n",
       "          [-1.5547,  1.1833,  1.0514,  ...,  1.5616,  0.3857, -0.1192],\n",
       "          ...,\n",
       "          [ 0.8113,  1.1363,  0.0513,  ..., -1.4856,  0.1663,  1.1950],\n",
       "          [-0.7103, -1.1890, -1.4732,  ..., -0.3617,  1.2559, -0.3068],\n",
       "          [-1.0540,  1.3824,  0.8412,  ..., -1.5822, -1.8532, -0.1889]],\n",
       "\n",
       "         [[-0.4256, -1.4169, -1.1269,  ...,  1.0230, -1.7283,  0.8330],\n",
       "          [ 0.6785, -0.7212, -1.1370,  ...,  0.2221,  0.0810, -0.9466],\n",
       "          [-1.2338, -1.7120, -0.3209,  ..., -1.6318,  0.7724,  0.5411],\n",
       "          ...,\n",
       "          [-0.3597, -0.7053, -1.5533,  ..., -0.1747,  0.4979,  0.4220],\n",
       "          [ 1.1039,  1.1657, -0.4862,  ..., -0.0743,  0.8213, -1.5858],\n",
       "          [ 0.0455, -0.7202, -0.4967,  ..., -1.2854, -0.6743,  0.9451]],\n",
       "\n",
       "         [[ 0.5514, -1.0287, -0.6694,  ..., -0.2551,  0.4673, -1.7599],\n",
       "          [-0.6026,  1.1654, -0.0329,  ..., -0.6553, -0.5549,  0.6661],\n",
       "          [ 1.4511,  1.1502,  1.5537,  ..., -0.6480,  0.5818, -0.3538],\n",
       "          ...,\n",
       "          [ 1.5945,  0.6723, -0.9946,  ...,  0.9152, -0.6649,  0.8947],\n",
       "          [ 0.5550,  1.3921, -0.9633,  ...,  1.0469,  0.5250, -1.7626],\n",
       "          [-0.6517,  1.6380,  1.3754,  ..., -0.3281, -0.8897, -0.0913]]],\n",
       "\n",
       "\n",
       "        [[[-1.6487, -0.0366,  0.9022,  ...,  1.2099, -1.1600,  1.6822],\n",
       "          [ 0.7001,  0.3535, -0.6918,  ..., -1.3961,  1.6153,  0.8375],\n",
       "          [ 1.0845,  0.8046,  0.9689,  ...,  0.0194,  1.3182,  1.1320],\n",
       "          ...,\n",
       "          [ 1.6232,  0.5718, -0.0305,  ...,  1.2236,  0.7945,  0.3300],\n",
       "          [-0.7823,  1.1524,  0.0167,  ..., -1.5121,  1.0706, -1.2822],\n",
       "          [-0.4745, -0.0775, -0.8160,  ..., -0.4709, -1.5858, -1.4895]],\n",
       "\n",
       "         [[-0.4887,  0.9018, -1.2742,  ..., -1.3290, -1.6829,  1.5602],\n",
       "          [ 1.1337, -0.8388, -0.7296,  ...,  0.4754, -0.6722, -0.9337],\n",
       "          [ 1.4280, -0.2013,  0.0901,  ..., -0.9301, -1.5903, -0.7651],\n",
       "          ...,\n",
       "          [ 1.3741, -0.4229, -0.3972,  ..., -1.2618,  1.3574, -1.4392],\n",
       "          [ 1.4265, -0.3612,  0.2589,  ...,  0.9416, -1.0653, -1.3880],\n",
       "          [ 1.2329, -1.3763, -0.1405,  ...,  1.3956,  1.3441, -1.3283]],\n",
       "\n",
       "         [[-1.2807, -1.3998,  0.0369,  ..., -1.6025, -0.6995,  0.2626],\n",
       "          [-1.2985, -1.4717, -1.5694,  ...,  0.9957, -1.1475, -1.2484],\n",
       "          [-1.5932, -1.3276, -0.5907,  ...,  0.1085, -1.1155,  1.0561],\n",
       "          ...,\n",
       "          [-0.0488,  1.6237, -0.5720,  ..., -0.3558, -0.7898, -0.7482],\n",
       "          [-0.9584,  0.3300,  0.3849,  ...,  1.6336, -1.6316,  0.6488],\n",
       "          [ 1.5454,  0.1751,  0.8348,  ...,  1.1524,  1.4314,  0.5376]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
